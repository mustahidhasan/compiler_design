function  inputText(argument Test_data){
	#Reads the sentence and return for tokenizing
	return sentence
end
}

function  separate_tokens(argument sentence ){
	#Takes the sentence and split it into words and return it
	return tokens
end
}

function filter_tokens(argument wordlist){
	Validword = []
	For i in range(len(wordlist))
		If word in wordlist is hightfrequency
			then
			      Validword.append(word)
	Endfor

return Validword
end
}


function  data_preprocess(argument dataset){
#Normalized the data set labels and convert them into NumPy array as machine-readable
	
createModel(argument numpy_array)
end
}

function createModel(argument numpy_array){
	x_train, x_test, y_train, y_test
	Separate the normalized data into train and test 
	train_Model(x_train, x_test, y_train, y_test)
end
}

function train_Model(argument numpy_array, argument numpy_array, argument numpy_array,argument numpy_array){

  model = Sequential()
  model.add(Embedding(vocab_size, 50, input_length=seq_length))
  model.add(LSTM(100, return_sequences=True))
  model.add(LSTM(100))
  model.add(Dense(100,activation='relu'))
  model.add(Dense(vocab_size, activation='softmax'))

  model.compile(loss = 'categorical_crossentropy', optimizer = 'adam',    metrics=['accuracy'])

  model.fit(X, y, batch_size=128, epochs=400)



#This step will train the model for prediction
end
}



function predict(){
  Test_data
  sentence= inputText(argument Test_data)
  Words = separate_tokens(argument sentence)
  valid _words = filter_tokens(argument Words )
  

#With the word matrix, the model will sort out the closest meaning full values and merge them together for getting the recommended word,
end
}


function main(){
  #Data_set is a .txt file with sonet 
  data_preprocess(argument Data_set )
 
  predict()

	
end
}

